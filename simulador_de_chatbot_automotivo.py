# -*- coding: utf-8 -*-
"""Simulador_de_ChatBot_Automotivo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YvNtfd3Hp5NPkaD5AsUAoUKqyeczydaT

#Simulador de ChatBot com GenAI e VectorDB

Este projeto apresenta um assistente virtual para a iAutos capaz de tirar d√∫vidas sobre regras, fraudes e an√∫ncios. O Chatbot combina busca sem√¢ntica em documentos PDF para quest√µes complexas e regras de neg√≥cio pr√©-definidas para perguntas sobre valores de ve√≠culos, garantindo que o usu√°rio receba sempre a informa√ß√£o correta, seja ela extra√≠da do manual ou uma diretriz da plataforma.

Desenvolvimento:
"""

# Instala√ß√£o das bibliotecas necess√°rias
!pip install langchain langchain-openai langchain-community chromadb pypdf unstructured -q

print("Bibliotecas instaladas com sucesso!")

# Importa√ß√£o das bibliotecas

from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
import os
from google.colab import userdata

# Configura√ß√£o segura das credenciais
os.environ["AZURE_OPENAI_ENDPOINT"] = userdata.get('AZURE_OPENAI_ENDPOINT')
os.environ["AZURE_OPENAI_API_KEY"] = userdata.get('AZURE_OPENAI_API_KEY')

# Vari√°veis de configura√ß√£o para os modelos
LLM_DEPLOYMENT_MODEL = userdata.get('LLM_DEPLOYMENT_MODEL')
LLM_API_VERSION = userdata.get('LLM_API_VERSION')
EMB_DEPLOYMENT_MODEL = userdata.get('EMB_DEPLOYMENT_MODEL')
EMB_API_VERSION = userdata.get('EMB_API_VERSION')

# E na sua inicializa√ß√£o do LLM/Embedding, use as vari√°veis de ambiente
# (O LangChain faz isso automaticamente se voc√™ definir as vari√°veis de ambiente)

# Exemplo para a FUN√á√ÉO 3 e 4:
embeddings = AzureOpenAIEmbeddings(
    azure_deployment=EMB_DEPLOYMENT_MODEL,
    openai_api_version=EMB_API_VERSION

)

llm = AzureChatOpenAI(
    azure_deployment=LLM_DEPLOYMENT_MODEL,
    openai_api_version=LLM_API_VERSION,
    temperature=0.3

)

#  Bibliotecas Importa√ß√µes Corrigidas
import langchain

# 1. Modelos (LLM e Embedding)
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings

# 2. Componentes Comunit√°rios (Loaders, Vectorstores)
from langchain.memory import ConversationBufferMemory
from langchain_community.vectorstores import Chroma

# 3. Componentes de Divis√£o de Texto (Text Splitter)
from langchain_text_splitters import RecursiveCharacterTextSplitter

# 4. Componentes Chave Refatorados (AJUSTADO):
# Chain volta para o m√≥dulo principal, onde geralmente fica:
from langchain.chains import ConversationalRetrievalChain #De volta ao 'langchain.chains'
# Memory e Prompts/Schema permanecem no Core ou Community:
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    SystemMessagePromptTemplate,
    MessagesPlaceholder
)
from langchain_core.documents import Document

# importando as bibliotecas para Azure
from langchain_openai import AzureChatOpenAI
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores import DocArrayInMemorySearch
from langchain.chains import RetrievalQA
from langchain_core.documents import Document

#  Imports necess√°rios para esta c√©lula
import langchain
import os
from google.colab import userdata
from langchain_openai import AzureChatOpenAI

# Ativa logs de debug
flg_log_debug = False
langchain.debug = flg_log_debug

# --- Configura√ß√£o Segura (Usando Secrets) ---
# O LangChain vai ler as chaves do ambiente automaticamente
os.environ["AZURE_OPENAI_ENDPOINT"] = userdata.get('AZURE_OPENAI_ENDPOINT')
os.environ["AZURE_OPENAI_API_KEY"] = userdata.get('AZURE_OPENAI_API_KEY')

# --- Configura√ß√µes Corretas do Modelo de CHAT ---
# Pegamos o nome do deployment do LLM (ex: "gpt-4.1") dos seus secrets
LLM_CHAT_DEPLOYMENT_NAME = userdata.get('LLM_DEPLOYMENT_MODEL')
LLM_CHAT_API_VERSION = userdata.get('LLM_API_VERSION')

# --- Inicializa o modelo de CHAT (LLM) via Azure ---
llm = AzureChatOpenAI(
    # O endpoint e a api_key j√° foram lidos das vari√°veis de ambiente
    deployment_name = LLM_CHAT_DEPLOYMENT_NAME,
    openai_api_version = LLM_CHAT_API_VERSION,
    temperature = 0.5,
    verbose = flg_log_debug
)

print("‚úÖ Modelo LLM (AzureChatOpenAI) inicializado com sucesso!")

# Download do arquivo de contexto
import os
import gdown

# 1. ID do arquivo no Google Drive
file_id = "1-ZpUOl8OA4lxa8CJ6auT42hSxaF3jclk"

#https://drive.google.com/file/d/1-ZpUOl8OA4lxa8CJ6auT42hSxaF3jclk
# 2. Nome do arquivo local
local_filename = "Caso de uso - Marketplace de classificados ve√≠culos.pdf"

# 3. Baixa o arquivo do Google Drive para a sess√£o local
gdown.download(f"https://drive.google.com/uc?id={file_id}", local_filename, quiet=False)

# Loader espec√≠fico para PDF
from langchain_community.document_loaders import PyPDFLoader

# O pacote PyPDFLoader √© espec√≠fico para PDF
arquivo_pdf = local_filename
loader = PyPDFLoader(arquivo_pdf)

# Carregando o contexto do PDF
documents_pdf = loader.load()
## O loader j√° retorna a lista de Document (um por p√°gina do PDF)

# Quantidade de p√°ginas
len(documents_pdf)

print(f"O PDF possui {len(documents_pdf)} p√°ginas.")

# Extract text content from the loaded PDF documents
text_content = ""
for doc in documents_pdf:
    text_content += doc.page_content + "\n"

# Split the text into chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 1000,
    chunk_overlap = 200,
    length_function = len,
)

chunks = text_splitter.split_text(text_content)

# Convert the text chunks back to Document objects
documents = [Document(page_content=t) for t in chunks]

print(f"O contexto foi dividido em {len(chunks)} documentos (chunks).")

# Importa√ß√£o das Bibliotecas (Vers√£o Azure) ---

print("Importando as bibliotecas necess√°rias (Vers√£o Azure)...")

# --- Bibliotecas de Prepara√ß√£o de Dados (N√≠vel 2) ---
from langchain_community.document_loaders import PyPDFLoader # Para carregar o PDF
from langchain.text_splitter import RecursiveCharacterTextSplitter # Para dividir o texto em peda√ßos (chunks)
# -----------------------------------------------------------------
# Usando o modelo de Embedding espec√≠fico do Azure
from langchain_openai import AzureOpenAIEmbeddings # Para transformar texto em vetores (n√∫meros)
# -----------------------------------------------------------------
from langchain_community.vectorstores import Chroma # O banco de dados vetorial

# --- Bibliotecas do Chatbot (N√≠vel 3) ---
# -----------------------------------------------------------------
# Usando o modelo de Chat espec√≠fico do Azure
from langchain_openai import AzureChatOpenAI # O "c√©rebro" do chatbot (LLM)
# -----------------------------------------------------------------
from langchain.chains import ConversationalRetrievalChain # A l√≥gica que conecta tudo (pergunta + vetores + LLM)

print("‚úÖ Todas as bibliotecas (Vers√£o Azure) foram importadas!")

# Processamento de Chunks e Cria√ß√£o do VectorDB
import os
from google.colab import userdata

print("Iniciando a cria√ß√£o do VectorDB (Vers√£o Azure)...")

# --- Configura√ß√£o Segura (Usando Secrets) ---
# Define as vari√°veis de ambiente que o LangChain procura
os.environ["AZURE_OPENAI_ENDPOINT"] = userdata.get('AZURE_OPENAI_ENDPOINT')
os.environ["AZURE_OPENAI_API_KEY"] = userdata.get('AZURE_OPENAI_API_KEY')

# 1. Inicializando o modelo de embedding (lendo secrets automaticamente)
print("   Inicializando o modelo de embedding (AzureOpenAIEmbeddings)...")
embeddings = AzureOpenAIEmbeddings(
    azure_deployment=userdata.get('EMB_DEPLOYMENT_MODEL'),
    openai_api_version=userdata.get('EMB_API_VERSION')
)

# 2. "Purifica√ß√£o" dos dados (Chunks)
# (Assumindo que a vari√°vel 'chunks' foi definida na c√©lula anterior)
print(f"   Filtrando os {len(chunks)} chunks (removendo nulos ou vazios)...")

chunk_texts = [
    doc.page_content for doc in documents
    if doc.page_content is not None and doc.page_content.strip() != ""
]
print(f"   ... {len(chunks)} chunks originais filtrados em {len(chunk_texts)} chunks de texto v√°lidos.")

# 3. Cria√ß√£o do VectorDB (ChromaDB)
print("   Criando o VectorDB (usando Chroma.from_texts)...")
vectordb = Chroma.from_texts(
    texts=chunk_texts,    # Nossa lista de texto puro
    embedding=embeddings  # O modelo de embedding da AZURE
)

print("\n‚úÖ VectorDB (Vectordb) criado com sucesso!")
print("   O 'vectordb' est√° pronto para ser consultado.")
print("\n PREPARA√á√ÉO DE DADOS CONCLU√çDA ---")

#  Cria√ß√£o da Cadeia (Chain) Conversacional
import os
from google.colab import userdata
from langchain.chains import ConversationalRetrievalChain
from langchain_openai import AzureChatOpenAI

print("Iniciando a cria√ß√£o do ChatBot (ConversationalRetrievalChain)...")

# --- Configura√ß√£o Segura (Usando Secrets) ---
# Define as vari√°veis de ambiente que o LangChain procura
os.environ["AZURE_OPENAI_ENDPOINT"] = userdata.get('AZURE_OPENAI_ENDPOINT')
os.environ["AZURE_OPENAI_API_KEY"] = userdata.get('AZURE_OPENAI_API_KEY')

# 1. Inicializando o "C√©rebro" (AzureChatOpenAI)
print("   Inicializando o modelo de LLM (AzureChatOpenAI)...")
llm = AzureChatOpenAI(
    # Endpoint e Chave s√£o lidos automaticamente das vari√°veis de ambiente
    azure_deployment=userdata.get('LLM_DEPLOYMENT_MODEL'), # Pega o deployment de CHAT
    openai_api_version=userdata.get('LLM_API_VERSION'),    # Pega a vers√£o da API de CHAT
    temperature=0.3
)

# 2. Criando a Cadeia (Chain) que conecta os componentes
print("   Criando a ConversationalRetrievalChain...")
# (Assumindo que 'vectordb' foi criado na c√©lula anterior)
qa_chain = ConversationalRetrievalChain.from_llm(
    llm,
    retriever=vectordb.as_retriever() # Conecta a cadeia ao nosso VectorDB
)

print("\n‚úÖ CHATBOT CRIADO! (qa_chain)")
print("   O chatbot est√° pronto para receber perguntas.")
print("\n CRIA√á√ÉO DA CADEIA CONCLU√çDA ---")

# CONFIGURA√á√ÉO DO CONECTOR E ROTEADOR
from langchain.chains import ConversationalRetrievalChain

DEFAULT_FALLBACK = (
    "Desculpe, a informa√ß√£o exata sobre este t√≥pico n√£o est√° detalhada "
    "nas Pol√≠ticas de Uso da iAutos que eu tenho acesso."
)

# Cria√ß√£o da chain principal
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectordb.as_retriever(search_kwargs={"k": 3}),
    return_source_documents=False
)

# --- Fun√ß√£o de roteamento inteligente ---
def handle_query(query):
    """
    Direciona a pergunta para o lugar certo.
    """
    q_lower = query.lower()

    # Detecta temas fora do escopo
    if any(palavra in q_lower for palavra in ["pre√ßo", "valor", "quanto custa", "gol", "caro", "barato"]):
        return "A iAutos n√£o define pre√ßos de ve√≠culos. Os valores variam conforme os an√∫ncios de vendedores. Consulte a barra de busca da plataforma para ver os pre√ßos atualizados."

    # Se for d√∫vida sobre pol√≠ticas, envia ao modelo
    result = qa_chain.invoke({"question": query, "chat_history": []})
    return result["answer"]

print("‚úÖ‚úÖ‚úÖ CONECTOR CRIADO E ROTEADOR ATIVO!")

# BATERIA DE TESTES AUTOM√ÅTICOS
def run_pipeline_tests(verbose=True):
    tests = [
        {"query": "O que √© a iAutos?", "expect_contains": ["marketplace", "classificados", "iAutos"]},
        {"query": "Quais s√£o os tipos de ve√≠culos permitidos?", "expect_contains": ["autom√≥veis", "motos", "utilit√°rios"]},
        {"query": "Qual o pre√ßo de um Gol?", "expect_contains": ["n√£o define pre√ßos", "an√∫ncios", "valores variam"]},  # <- Novo teste
        {"query": "Como eu entro em contato com o vendedor?", "expect_contains": ["contato", "vendedor", "mensagem"]},
        {"query": "O que acontece se eu fraudar?", "expect_contains": ["banido", "banimento", "fraudar"]}
    ]

    passed, failed = 0, 0
    details = []

    print("\n--- üß™ Iniciando Bateria de Testes Autom√°ticos ---\n")

    for t in tests:
        q = t["query"]
        try:
            ans = handle_query(q)
            ans_lower = ans.lower()
            expected = any(word in ans_lower for word in t["expect_contains"])

            if expected:
                passed += 1
                status = "‚úÖ PASSOU"
            else:
                failed += 1
                status = "‚ùå FALHOU"

            print(f"[{status}] Pergunta: {q}\n--> Resposta: {ans[:100]}...\n")
            details.append({"query": q, "status": status, "answer_sample": ans[:100]})

        except Exception as e:
            failed += 1
            print(f"[ERRO] Pergunta: {q} -> {e}\n")
            details.append({"query": q, "status": "ERRO", "error": str(e)})

    print(f"--- üèÅ Resumo dos Testes ---")
    print(f"‚úÖ Passou: {passed} | ‚ùå Falhou: {failed}\n")
    return passed, failed, details

# --- Executa ---
run_pipeline_tests()

!pip install gradio -q

# (Certifique-se de que a c√©lula anterior, que define 'qa_chain', j√° foi executada)

import gradio as gr

def interface_chatbot_robusta(message, history):
    """
    Esta √© a fun√ß√£o que o Gradio vai chamar.
    Ela usa EXATAMENTE a mesma l√≥gica do seu loop 'simular_conversa'.
    """

    # 1. O ROTEADOR (que lida com o "Gol")
    q_lower = message.lower()
    if any(palavra in q_lower for palavra in ["pre√ßo", "valor", "quanto custa", "gol", "caro", "barato"]):

        # A√ß√£o A: Resposta padr√£o
        return "A iAutos n√£o define pre√ßos de ve√≠culos. Os valores variam conforme os an√∫ncios de vendedores. Consulte a barra de busca da plataforma para ver os pre√ßos atualizados."

    else:
        # A√ß√£o B: Chama o RAG (qa_chain)
        # A 'qa_chain' j√° tem mem√≥ria interna, ent√£o n√£o precisamos passar o 'history'
        result = qa_chain.invoke({"question": message})
        return result["answer"]

# --- FIM DA L√ìGICA ---


# 2. CRIAR E LAN√áAR A P√ÅGINA
print("Iniciando a p√°gina de teste interativa...")
print("Aguarde o link p√∫blico (.gradio.live) aparecer...")

demo_chat = gr.ChatInterface(
    fn=interface_chatbot_robusta,   # A fun√ß√£o que acabamos de criar
    title="ü§ñ iAutos ChatBot (Vers√£o Robusta)",
    description="Teste o bot com perguntas de pol√≠tica ('fraude') ou fora do escopo ('pre√ßo do Gol').",
    examples=["O que √© a iAutos?", "O que acontece se eu fraudar?", "Qual o pre√ßo de um Gol?"],
    chatbot=gr.Chatbot(height=500)
)

# 3. INICIAR (share=True cria um link p√∫blico tempor√°rio)
demo_chat.launch(share=True)

#  Interrogar o chatbot (O Teste de Batalha) ---

flg_log_debug = False
langchain.debug = flg_log_debug

print("O Chatbot est√° pronto. Fa√ßa suas perguntas sobre o PDF da iAutos.")
print("Digite 'sair' a qualquer momento para encerrar o teste.")

# O Chatbot precisa de uma "mem√≥ria de curto prazo" (uma lista vazia)
chat_history = []

while True:
    # 1.  (voc√™) faz a pergunta
    query = input("\n[VOC√ä]: ")

    # 2. Condi√ß√£o de Sa√≠da
    if query.lower() == 'sair':
        print("\n[CHATBOT]: At√© logo! Obrigado por usar a iAutos.")
        break
    # 3. O Chatbot "pensa"
    #    Ele usa a 'query' (pergunta) e a 'chat_history' (mem√≥ria)
    result = qa_chain.invoke({"question": query, "chat_history": chat_history})

    # 4. O Chatbot "lembra" (atualiza a mem√≥ria)
    chat_history.append((query, result["answer"]))

    # 5. O Chatbot "fala" (imprime a resposta)
    print("\n[chatbot]:", result['answer'])



"""##**2. Processo final**

Desenvolva aqui:
"""

!pip install langchain langchain-openai langchain_community pypdf chromadb tiktoken faiss-cpu gdown --quiet

## ==============================================================================
## SE√á√ÉO 2: PROCESSO FINAL (TUDO EM UMA C√âLULA)
## ==============================================================================

## --- 1. Importa√ß√µes necess√°rias ---
import os
import gdown
from google.colab import userdata
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts.chat import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    SystemMessagePromptTemplate,
    MessagesPlaceholder
)
import warnings

# Ignorar avisos de deprecia√ß√£o do LangChain (opcional, mas limpa o output)
warnings.filterwarnings("ignore")

## --- 2. Defini√ß√£o de Todas as Fun√ß√µes ---

def baixar_documento_contexto():
    """Baixa o arquivo PDF do Google Drive."""
    file_id = "1-ZpUOl8OA4lxa8CJ6auT42hSxaF3jclk"
    local_filename = "iAutos_Politicas_e_Uso.pdf"
    print(f"\nüì• Baixando arquivo de contexto: {local_filename}")
    try:
        gdown.download(f"https://drive.google.com/uc?id={file_id}", local_filename, quiet=True)
        print("‚úÖ Arquivo baixado com sucesso.")
        return local_filename
    except Exception as e:
        print(f"‚ùå Erro ao baixar o arquivo: {e}")
        return None

def preparar_documentos(pdf_path):
    """Carrega o PDF e divide em chunks."""
    if not pdf_path:
        return []
    print("\nüìÑ Carregando e fatiando o documento...")
    loader = PyPDFLoader(pdf_path)
    documents_pdf = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
    )
    chunks = text_splitter.split_documents(documents_pdf)
    print(f"‚úÖ Documento dividido em {len(chunks)} fragmentos.")
    return chunks

def criar_vectordb(chunks):
    """Cria o ChromaDB usando Azure Embeddings (com Secrets)."""
    print("\nüß† Criando o VectorDB (ChromaDB) com Azure Embeddings...")
    os.environ["AZURE_OPENAI_ENDPOINT"] = userdata.get('AZURE_OPENAI_ENDPOINT')
    os.environ["AZURE_OPENAI_API_KEY"] = userdata.get('AZURE_OPENAI_API_KEY')

    embeddings = AzureOpenAIEmbeddings(
        azure_deployment=userdata.get('EMB_DEPLOYMENT_MODEL'),
        openai_api_version=userdata.get('EMB_API_VERSION')
    )
    vectordb = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        collection_name="iAutos_politicas"
    )
    print("‚úÖ ChromaDB criado e pronto para recupera√ß√£o (Retriever).")
    return vectordb

def construir_chatbot_chain(vectordb):
    """Constr√≥i o ChatBot com LLM, Mem√≥ria e o Prompt corrigido."""
    print("\nü§ñ Construindo o Chatbot (ConversationalRetrievalChain)...")

    os.environ["AZURE_OPENAI_ENDPOINT"] = userdata.get('AZURE_OPENAI_ENDPOINT')
    os.environ["AZURE_OPENAI_API_KEY"] = userdata.get('AZURE_OPENAI_API_KEY')

    llm = AzureChatOpenAI(
        azure_deployment=userdata.get('LLM_DEPLOYMENT_MODEL'),
        openai_api_version=userdata.get('LLM_API_VERSION'),
        temperature=0.3
    )

    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer"
    )

    # --- PROMPT COMPAT√çVEL COM STUFFDOCUMENTSCHAIN ---
    from langchain.prompts import PromptTemplate

    SYSTEM_TEMPLATE = """
Voc√™ √© o ChatBot de Atendimento da iAutos, um marketplace de classificados de ve√≠culos.
Sua miss√£o √© responder perguntas dos clientes de forma precisa, educada e SOMENTE com base no contexto
fornecido abaixo, que cont√©m as pol√≠ticas de uso e regras de publica√ß√£o de ve√≠culos da iAutos.

Diretrizes de Resposta:
- Responda SOMENTE com informa√ß√µes encontradas no contexto recuperado.
- Se o contexto **n√£o contiver** a resposta, diga EXATAMENTE:
  "Desculpe, a informa√ß√£o exata sobre este t√≥pico n√£o est√° detalhada nas Pol√≠ticas de Uso da iAutos que eu tenho acesso."
- Nunca invente ou fa√ßa suposi√ß√µes, mesmo que a pergunta pare√ßa simples.
- Mantenha o tom de voz profissional, educado e prestativo.

Contexto:
{context}

Pergunta:
{question}
"""

    qa_prompt = PromptTemplate(
        template=SYSTEM_TEMPLATE,
        input_variables=["context", "question"]
    )

    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectordb.as_retriever(search_kwargs={"k": 3}),
        memory=memory,
        combine_docs_chain_kwargs={"prompt": qa_prompt},
        return_source_documents=True,
        output_key="answer"
    )

    print("‚úÖ ChatBot pronto!")
    return qa_chain

def simular_conversa(qa_chain):
    """
    Loop de conversa√ß√£o para interagir com o ChatBot.
    AGORA COM O ROTEADOR INTELIGENTE.
    """
    print("\n\n--- ü§ñ SIMULADOR DE CHATBOT iAutos (Vers√£o Robusta) ---")
    print("Digite 'sair' a qualquer momento para encerrar.\n")

    # A mem√≥ria j√° est√° dentro da qa_chain, ent√£o n√£o precisamos
    # gerenciar o hist√≥rico aqui fora.

    while True:
        try:
            query = input("[VOC√ä]: ")
            if query.lower() in ["sair", "exit", "tchau", "fim", "encerrar"]:
                print("\n[CHATBOT]: At√© logo! Obrigado por usar a iAutos.")
                break

            # --- ROTEADOR INTELIGENTE (A l√≥gica do teste) ---
            q_lower = query.lower()
            if any(palavra in q_lower for palavra in ["pre√ßo", "valor", "quanto custa", "gol", "caro", "barato"]):
                # A√á√ÉO A: Responde com a regra de neg√≥cio
                print("\n[CHATBOT]: A iAutos n√£o define pre√ßos de ve√≠culos. Os valores variam conforme os an√∫ncios de vendedores. Consulte a barra de busca da plataforma para ver os pre√ßos atualizados.")

            else:
                # A√á√ÉO B: Usa o RAG para perguntas de pol√≠tica
                result = qa_chain.invoke({"question": query})
                print("\n[CHATBOT]:", result['answer'])
            # --- FIM DO ROTEADOR ---

        except Exception as e:
            print(f"\n[CHATBOT]: Ops, houve um erro inesperado. Erro: {e}")
            break # Encerra o loop em caso de erro# Encerra o loop em caso de erro

def rodar_pipeline_completo():
    """Executa todas as etapas do processo final."""

    # A instala√ß√£o foi movida para a c√©lula anterior

    pdf_path = baixar_documento_contexto()
    if pdf_path:
        chunks = preparar_documentos(pdf_path)
        if chunks:
            vectordb = criar_vectordb(chunks)
            qa_chain = construir_chatbot_chain(vectordb)
            simular_conversa(qa_chain)
        else:
            print("‚ùå Erro: Nenhum fragmento (chunk) foi criado do PDF.")
    else:
        print("‚ùå Erro: O download do PDF falhou. O pipeline n√£o pode continuar.")

## --- 3. Execu√ß√£o Imediata ---
# (Roda o pipeline assim que esta c√©lula √© executada)
rodar_pipeline_completo()

"""# Exemplos de Conversa

--- ü§ñ SIMULADOR DE CHATBOT iAutos ---
Digite 'sair' a qualquer momento para encerrar.

[VOC√ä]: Quais s√£o os tipos de ve√≠culos permitidos?

[CHATBOT]: Os tipos de ve√≠culos permitidos na iAutos s√£o:

- Passageiros: autom√≥veis (sedan, hatch, SUV, picape, minivan), van, furg√£o, micro-√¥nibus, √¥nibus, motonetas, motocicletas, triciclos e quadriciclos.
- Carga: utilit√°rios, caminh√µes e camionetas.
- Motoriza√ß√£o: ve√≠culos de combust√£o, el√©tricos e h√≠bridos.

[VOC√ä]: E sobre as motos, quais as regras?

[CHATBOT]: Na iAutos, √© permitido anunciar motonetas, motocicletas, triciclos e quadriciclos. No entanto, ciclomotores (ve√≠culos de duas ou tr√™s rodas com at√© 50 cm¬≥ ou equivalente el√©trico e velocidade m√°xima de at√© 50 km/h) s√£o proibidos.

Al√©m disso, nas fotos ou descri√ß√µes dos an√∫ncios, n√£o √© permitido incluir n√∫meros de telefone, celular, WhatsApp, e-mails ou endere√ßos de localiza√ß√£o (logradouros).

[VOC√ä]: Qual o pre√ßo m√©dio de um Gol ?

[CHATBOT]: Desculpe, a informa√ß√£o exata sobre este t√≥pico n√£o est√° detalhada nas Pol√≠ticas de Uso da iAutos que eu tenho acesso.

[VOC√ä]: Como eu entro em contato com o vendedor?

[CHATBOT]: O contato entre comprador e vendedor na iAutos s√≥ √© liberado pela plataforma ap√≥s solicita√ß√£o de interesse (lead) pelo comprador. Portanto, primeiro √© necess√°rio demonstrar interesse no ve√≠culo pela plataforma; ap√≥s isso, o contato ser√° disponibilizado.

[VOC√ä]: O que acontece se eu tentar fraudar a plataforma?

[CHATBOT]: Se voc√™ tentar fraudar a plataforma iAutos, a puni√ß√£o aplicada √© o banimento permanente da conta.

[VOC√ä]: sair

[CHATBOT]: At√© logo! Obrigado por usar a iAutos.
"""